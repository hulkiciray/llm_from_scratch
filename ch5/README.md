# Chapter 5: Pretraining on Unlabeled Data

This chapter we will dive deeper into the training phase of LLMs. The topics we are going to cover are as follow;

- Evaluating generative text models
  - Using GPT to generate text
  - Calculating the text generation loss
  - Calculating the training and validation set losses
- Training an LLM
- Decoding strategies to control randomness
  - Temperature scaling
  - Top-k sampling
  - Modifying the text generation function
- Loading and saving model weights in PyTorch
- Loading pretrained weights from OpenAI

### Summary of the Chapter
- When LLMs generate text, they output one token at a time.
- By default, the next token is generated by converting the model outputs into probability scores and selecting the token from the vocabulary that corresponds to the highest probability score, which is known as **"greedy decoding".**
- Using **probabilistic sampling** and **temperature scaling**, we can influence the diversity and coherence of the generated text.
- Training and validation set losses can be used to gauge the quality of text generated by LLM during training.
- Pretraining an LLM involves changing its weights to minimize the training loss.
- The training loop for LLMs itself is a standard procedure in deep learning, using a conventional cross-entropy loss and AdamW optimizer.
- Pretraining an LLM on a large text corpus is time and resource intensive, so we can load openly available weights as an alternative to pretraining the model on a large dataset ourselves.