{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implementing a GPT Model from Scratch to Generate Text\n",
    "\n",
    "**Content**\n",
    "- Coding a GPT-like LLM that can be trained to generate human-like text.\n",
    "- Normalizing layer activations to stabilize neural network training.\n",
    "- Adding shortcut connections in deep neural networks.\n",
    "- Implementing transformer blocks to create GPT models of various sizes.\n",
    "- Computing the number of parameters and storage requirements of GPT models."
   ],
   "id": "59d314847447645a"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "We start with specifying the configuration of the small GPT-2 model via the following Python dictionary.",
   "id": "73ee6b3daf406598"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T21:40:55.264663Z",
     "start_time": "2025-02-07T21:40:55.260089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ],
   "id": "3ab501539c1eee4a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **vocab_size** refers to a vocabulary of 50,257 words, as used by the BPE tokenizer (see chapter 2).\n",
    "- **context_length** denotes the maximum number of input tokens the model can handle via the positional embeddings (see chapter 2).\n",
    "- **emb_dim** represents the embedding size, transforming each token into a 768-dimensional vector.\n",
    "- **n_heads** indicates the count of attention heads in the multi-head attention mechanism (see chapter 3).\n",
    "- **n_layers** specifies the number of transformer blocks in the model, which we will cover in the upcoming discussion.\n",
    "- **drop_rate** indicates the intensity of the dropout mechanism (0.1 implies a 10% random drop out of hidden units) to prevent overfitting (see chapter 3).\n",
    "- **qkv_bias** determines whether to include a bias vector in the Linear layers of the multi-head attention for query, key, and value computations. We will initially disable this, following the norms of modern LLMs, but we will revisit it in chapter 6 when we load pretrained GPT-2 weights from OpenAI into our model (see chapter 6)."
   ],
   "id": "e49570b30287897f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T23:52:22.752578Z",
     "start_time": "2025-02-07T23:52:20.020788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ],
   "id": "f3214753db5e4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**The DummyGPTModel class in this code defines a simplified version of a GPT-like model using PyTorch’s neural network module (nn.Module). The model architecture in the DummyGPTModel class consists of token and positional embeddings, dropout, a series of transformer blocks (DummyTransformerBlock), a final layer normalization (DummyLayerNorm), and a linear output layer (out_head). The configuration is passed in via a Python dictionary, for instance, the GPT_CONFIG_124M dictionary we created earlier.**",
   "id": "e6962d29ae22ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T10:34:07.949969Z",
     "start_time": "2025-02-08T10:34:07.685241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ],
   "id": "d1e78c7308d86078",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hulkiciray/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "The resulting token IDs for the two texts are above.\n",
    "\n",
    "Next, we initialize a new 124M parameter DummyGPTModel instance and feed it the tokenized batch."
   ],
   "id": "3a045641a9e828f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T10:44:21.194040Z",
     "start_time": "2025-02-08T10:44:20.739660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ],
   "id": "4bdd9d043a243e1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**The output tensor has two rows corresponding to the two text samples. Each text sample consists of four tokens; each token is a 50,257-dimensional vector, which matches the size of the tokenizer’s vocabulary.**\n",
    "\n",
    "**The embedding has 50,257 dimensions because each of these dimensions refers to a unique token in the vocabulary. When we implement the postprocessing code, we will convert these 50,257-dimensional vectors back into token IDs, which we can then decode into words.**"
   ],
   "id": "7e36f46adc5104b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T10:53:43.922989Z",
     "start_time": "2025-02-08T10:53:43.915143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ],
   "id": "34707ce768770fe2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "The neural network layer we have coded consists of a Linear layer followed by a nonlinear activation function, ReLU (short for rectified linear unit), which is a standard activation function in neural networks.\n",
    "\n",
    "Let’s examine the mean and variance:"
   ],
   "id": "ebc6321fcf8a47b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T10:56:17.641769Z",
     "start_time": "2025-02-08T10:56:17.636934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ],
   "id": "cd98d127be04e7fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T12:36:37.804039Z",
     "start_time": "2025-02-08T12:36:37.793810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ],
   "id": "996da48340bd0ded",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "As we can see based on the results, the normalized layer outputs, which now also contain negative values, have 0 mean and a variance of 1.\n",
    "\n",
    "Note that the value –5.9605e-08 in the output tensor is the scientific notation for –5.9605 × 10-8, which is –0.000000059605 in decimal form. This value is very close to 0."
   ],
   "id": "6729091e2f8c4df3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T12:41:19.369535Z",
     "start_time": "2025-02-08T12:41:19.365224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ],
   "id": "604c188ae4e6dcdc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "Do not spend much time on this.",
   "id": "775f3600eb37c63c"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "So far, we have coded and applied layer normalization in a step-by-step process. Let’s now encapsulate this process in a PyTorch module that we can use in the GPT model later.",
   "id": "51faa3d41675a8b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T12:44:44.199220Z",
     "start_time": "2025-02-08T12:44:44.196322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ],
   "id": "6ae97b09f7dae2b3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T15:07:06.249353Z",
     "start_time": "2025-02-08T15:07:06.240323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, keepdim=True, unbiased=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ],
   "id": "44075760e35fa3b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Implementing a feed forward network with GELU activations**\n",
    "\n",
    "GELU and SwiGLU are more complex and smooth activation functions incorporating Gaussian and sigmoid-gated linear units, respectively. They offer improved performance for deep learning models, unlike the simpler ReLU."
   ],
   "id": "435904ddb51f9300"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T15:15:13.888587Z",
     "start_time": "2025-02-08T15:15:13.884315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ],
   "id": "ac5762611fafc2d4",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T15:20:10.436167Z",
     "start_time": "2025-02-08T15:20:10.125106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8,3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label} (x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "2c8f5c8a33860f42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T15:36:44.103142Z",
     "start_time": "2025-02-08T15:36:44.100226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "id": "197386ef6056d179",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T15:36:45.186710Z",
     "start_time": "2025-02-08T15:36:45.162774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.randn(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ],
   "id": "c504b2af454b8c56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The shape of the output tensor is the same as that of the input tensor.\n",
    "\n",
    "**Next let's add residual (shortcut) connections. Shortcut connection creates an alternative, shorter path for the gradient to flow through the network by skipping one or more layers, which is achieved by adding the output of one layer to the output of a later layer. This is why these connections are also known as skip connections. They play a crucial role in preserving the flow of gradients during the backward pass in training.**"
   ],
   "id": "a7e173a2cacc3219"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T16:20:27.113358Z",
     "start_time": "2025-02-08T16:20:27.108156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ],
   "id": "5400ab0b05989cd2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "The code implements a deep neural network with five layers, each consisting of a linear layer and a GELU activation function.",
   "id": "47d9da9c1fe931d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T16:20:31.629387Z",
     "start_time": "2025-02-08T16:20:31.623428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)"
   ],
   "id": "df701a98439cd37a",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T16:24:04.036007Z",
     "start_time": "2025-02-08T16:24:04.032955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ],
   "id": "8c6705e262bef21f",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T16:24:19.290365Z",
     "start_time": "2025-02-08T16:24:19.244280Z"
    }
   },
   "cell_type": "code",
   "source": "print_gradients(model_without_shortcut, sample_input)",
   "id": "fdfe631ceb8d090e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
      "layers.3.0.weight has gradient mean of 0.0013988735154271126\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T20:38:35.781602Z",
     "start_time": "2025-02-08T20:38:35.665681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ],
   "id": "6e1e3288c29b0c1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "Shortcut connections are important for overcoming the limitations posed by the vanishing gradient problem in deep neural networks.\n",
    "\n",
    "We’ll connect all of the previously covered concepts (layer normalization, GELU activations, feed forward module, and shortcut connections) in a transformer block.\n",
    "\n",
    "Let’s implement the transformer block, a fundamental building block of GPT and other LLM architectures."
   ],
   "id": "ed9a7de351abb1d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T21:17:49.414299Z",
     "start_time": "2025-02-09T21:17:49.394305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from ch3.ch3_02 import MultiHeadAttention\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ],
   "id": "3d0120302b3b58b5",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T21:22:30.946136Z",
     "start_time": "2025-02-09T21:22:30.878881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ],
   "id": "625242bee31ac2b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "As we can see, the transformer block maintains the input dimensions in its output, indicating that the transformer architecture processes sequences of data without altering their shape throughout the network.\n",
    "\n",
    "The output is a context vector that encapsulates information from the entire input sequence"
   ],
   "id": "9aceb77fa35f6d9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### GPT Model\n",
    "\n",
    "Let’s now replace the DummyTransformerBlock and DummyLayerNorm placeholders with the real TransformerBlock and LayerNorm classes we coded previously to assemble a fully working version of the original 124-million-parameter version of GPT-2."
   ],
   "id": "ce42825c66f6df33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T23:48:10.895341Z",
     "start_time": "2025-02-09T23:48:10.885380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ],
   "id": "86efdd583bc5f080",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "The __init__ constructor of this GPTModel class initializes the token and positional embedding layers using the configurations passed in via a Python dictionary, cfg. These embedding layers are responsible for converting input token indices into dense vectors and adding positional information (see chapter 2).\n",
    "\n",
    "Next, the __init__ method creates a sequential stack of TransformerBlock modules equal to the number of layers specified in cfg. Following the transformer blocks, a LayerNorm layer is applied, standardizing the outputs from the transformer blocks to stabilize the learning process. Finally, a linear output head without bias is defined, which projects the transformer’s output into the vocabulary space of the tokenizer to generate logits for each token in the vocabulary.\n",
    "\n",
    "The forward method takes a batch of input token indices, computes their embeddings, applies the positional embeddings, passes the sequence through the transformer blocks, normalizes the final output, and then computes the logits, representing the next token’s unnormalized probabilities. We will convert these logits into tokens and text outputs in the next section."
   ],
   "id": "5fd07e1a814ef169"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T23:51:14.780589Z",
     "start_time": "2025-02-09T23:51:13.959807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ],
   "id": "c53fed38018ca1a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.4167, -0.0676, -0.2001,  ...,  0.3916,  0.1081, -0.1607],\n",
      "         [ 0.3593, -0.6993, -0.8715,  ..., -0.2451,  0.2673, -0.2367],\n",
      "         [ 1.0220,  0.0540, -0.3235,  ...,  0.0761, -0.5456, -0.1451],\n",
      "         [-0.8916,  0.4795, -0.2215,  ...,  0.6458,  0.3718,  0.1394]],\n",
      "\n",
      "        [[-0.4959, -0.2750, -0.0627,  ...,  0.2587,  0.1351, -0.3289],\n",
      "         [ 0.2549,  0.3801, -0.1802,  ...,  0.6806, -0.0931,  0.4824],\n",
      "         [ 1.1538,  0.7210, -0.2097,  ...,  0.8688,  0.1912, -0.2601],\n",
      "         [-0.0953,  0.5321,  0.2437,  ...,  1.2450, -0.1883, -0.0224]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**As we can see, the output tensor has the shape [2, 4, 50257], since we passed in two input texts with four tokens each. The last dimension, 50257, corresponds to the vocabulary size of the tokenizer. Later, we will see how to convert each of these 50,257-dimensional output vectors back into tokens.**",
   "id": "8b92481a64093262"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T23:53:19.747637Z",
     "start_time": "2025-02-09T23:53:19.740402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ],
   "id": "b4228cd6dcfc1f73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163009536\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "The result is not 124M. Why? It is because weight tying. Let's remove the output layer parameter count from the total GPT-2 model count according to the weight tying.",
   "id": "6ca5a2b22b22823d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T23:59:58.938693Z",
     "start_time": "2025-02-09T23:59:58.936158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2}\")"
   ],
   "id": "3b13d0d9941eadb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124412160\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "Now the parameter size is around 124M.\n",
    "\n",
    "We will revisit and implement the weight tying concept later in chapter 6 when we load the pretrained weights from OpenAI.\n",
    "\n",
    "Let's compute memory requirements of the 163M parameters in our GPTModel object:"
   ],
   "id": "fe634f55b6326535"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T00:02:07.836927Z",
     "start_time": "2025-02-10T00:02:07.833484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 ** 2)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ],
   "id": "ebc5fe867742c646",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "We can see that even for that tiny model, memory requirement is not small. Consider the modern billion parameter sized models. It is too much.",
   "id": "25c3e3991118290a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generating Text\n",
    "\n",
    "Now that we’ve implemented the GPTModel architecture and saw that it outputs numeric tensors of shape [batch_size, num_tokens, vocab_size], let’s write the code to convert these output tensors into text.\n",
    "\n",
    "The process by which a GPT model goes from output tensors to generated text involves several steps. These steps include decoding the output tensors, selecting tokens based on a probability distribution, and converting these tokens into human-readable text."
   ],
   "id": "ab6c9625378958a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T01:36:38.065046Z",
     "start_time": "2025-02-10T01:36:38.058560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ],
   "id": "43c6c2d124197ac",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "This code demonstrates a simple implementation of a generative loop for a language model using PyTorch.(crops the current context to fit the model’s maximum context size)\n",
    "\n",
    "When we implement the GPT training code in the next chapter, we will use additional sampling techniques to modify the softmax outputs such that the model doesn’t always select the most likely token. This introduces variability and creativity in the generated text.\n",
    "\n",
    "Let’s now try out the generate_text_simple function with the \"Hello, I am\" context as model input. First, we encode the input context into token IDs:"
   ],
   "id": "bd0eb6df098882b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T01:41:47.592477Z",
     "start_time": "2025-02-10T01:41:47.577504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ],
   "id": "cc214472020de759",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "Next, we put the model into .eval() mode. This disables random components like dropout, which are only used during training, and use the generate_text_simple function on the encoded input tensor:",
   "id": "ed13f9040cae62dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T01:44:17.418754Z",
     "start_time": "2025-02-10T01:44:16.996629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens = 6,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ],
   "id": "30412547256a5c06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018,  7283, 46275, 41426, 33167, 33239]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "Use decode to convert the token IDs back into text",
   "id": "9037c2929180fb94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T01:45:23.545535Z",
     "start_time": "2025-02-10T01:45:23.538142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ],
   "id": "bc7de8598f46f23e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Feature IT snowballProtect youngstersMu\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "As we can see, the model generated gibberish, which is not at all like the coherent text Hello, I am a model ready to help. What happened? The reason the model is unable to produce coherent text is that we haven’t trained it yet. So far, we have only implemented the GPT architecture and initialized a GPT model instance with initial random weights. Model training is a large topic in itself, and we will tackle it in the next chapter.",
   "id": "24e1138e9c3009bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "- **Layer normalization** stabilizes training by ensuring that each layer's outputs have a consistent mean and variance\n",
    "- Shortcut connections, in other words **residual connections** are connections that skip one or more layers by feeding the output of one layer directly to a deeper layer, which helps mitigate the **vanishing gradient problem** when training deep neural networks such as LLMs.\n",
    "- **Transformer** blocks are a core structural component of **GPT** models, combining **masked multi-head attention** modules with fully connected feed forward networks that use the **GELU** activation function\n",
    "- GPT models are LLMs with many repeated transformer blocks that have millions to billions of parameters.\n",
    "- GPT models come in **various sizes**, for example 124, 345, 762, 1542 million parameters which **we can implement** with the **same GPTModel** python class.\n",
    "- The text-generation capability of a GPT-like LLM involves **decoding** output tensors into human-readable text by sequentially predicting one token at a time based on a given input context.\n",
    "- **Without training**, a GPT model generates **incoherent text**, which underscores the importance of model training for coherent text generation."
   ],
   "id": "2262a111dcede341"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
