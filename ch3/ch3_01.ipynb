{ "cells": [  {   "metadata": {},   "cell_type": "markdown",   "source": [    "## Coding Attention Mechanisms\n",    "\n",    "Here we will build 4 different variants of attention mechanisms.\n",    "1. **Simplified self-attention:** A simplified self-attention technique to introduce the broader idea.\n",    "2. **Self-attention:** Self-attention with trainable weights that forms the basis of the mechanism used in LLMs.\n",    "3. **Causal attention:** Type of self-attention used in LLMs that allows a model to consider only previous and current inputs in a sequence.\n",    "4. **Multi-head attention:** An extention of self-attention and causal attention that enables the model to simultaneously attend to information from different representation subspaces.\n",    "\n",    "\n",    "Before the advent of transformers, recurrent neural networks (RNNs) were the most popular encoder–decoder architecture for language translation. Although RNNs work fine for translating short sentences, they don’t work well for longer texts as they don’t have direct access to previous words in the input.\n",    "\n",    "Self-attention is a mechanism that allows each position in the input sequence to consider the relevancy of, or “attend to,” all other positions in the same sequence when computing the representation of a sequence."   ],   "id": "87ce602a4633142e"  },  {   "metadata": {},   "cell_type": "markdown",   "source": "## Simplified Self-attention",   "id": "41e8186414760950"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T19:12:33.367247Z",     "start_time": "2025-02-06T19:12:32.498125Z"    }   },   "cell_type": "code",   "outputs": [],   "execution_count": 2,   "source": [    "import torch\n",    "inputs = torch.tensor(\n",    "    [[0.43, 0.15, 0.89], # Your (x^1)\n",    "     [0.55, 0.87, 0.66],  # journey (x^2)\n",    "     [0.57, 0.85, 0.64], # starts (x^3)\n",    "     [0.22, 0.58, 0.33], # with (x^4)\n",    "     [0.77, 0.25, 0.10], # one (x^5)\n",    "     [0.05, 0.80, 0.55]] # step (x^6)\n",    ")"   ],   "id": "6ba1a3170bc89910"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T19:14:46.183714Z",     "start_time": "2025-02-06T19:14:46.176502Z"    }   },   "cell_type": "code",   "source": [    "query = inputs[1]\n",    "attn_scores_2 = torch.empty(inputs.shape[0])\n",    "for i, x_i in enumerate(inputs):\n",    "    attn_scores_2[i] = torch.dot(x_i, query)\n",    "print(attn_scores_2)"   ],   "id": "ee97d7aad134aacd",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"     ]    }   ],   "execution_count": 3  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T19:20:04.937257Z",     "start_time": "2025-02-06T19:20:04.932471Z"    }   },   "cell_type": "code",   "source": [    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",    "print(\"Attention weights:\", attn_weights_2_tmp)\n",    "print(\"Sum:\", attn_weights_2_tmp.sum())"   ],   "id": "872a831c8c6237ac",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",      "Sum: tensor(1.0000)\n"     ]    }   ],   "execution_count": 4  },  {   "metadata": {},   "cell_type": "raw",   "source": [    "In practice, it’s more common and advisable to use the softmax function for normalization. This approach is better at managing extreme values and offers more favorable gradient properties during training.\n",    "\n",    "The following is a basic implementation of the softmax function for normalizing the attention scores:"   ],   "id": "54e63588efeda4e8"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T19:23:46.994872Z",     "start_time": "2025-02-06T19:23:46.988975Z"    }   },   "cell_type": "code",   "source": [    "def softmax_naive(x):\n",    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",    "\n",    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",    "print(\"Attention weights:\", attn_weights_2_naive)\n",    "print(\"Sum:\", attn_weights_2_naive.sum())"   ],   "id": "110c703d0b66ae03",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",      "Sum: tensor(1.)\n"     ]    }   ],   "execution_count": 5  },  {   "metadata": {},   "cell_type": "raw",   "source": "In practice, it’s advisable to use the PyTorch implementation of softmax, which has been extensively optimized for performance:",   "id": "69b93b5f626905d6"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T19:26:24.860252Z",     "start_time": "2025-02-06T19:26:24.854082Z"    }   },   "cell_type": "code",   "source": [    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",    "print(\"Attention weights:\", attn_weights_2)\n",    "print(\"Sum:\", attn_weights_2.sum())"   ],   "id": "4840c17152738657",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",      "Sum: tensor(1.)\n"     ]    }   ],   "execution_count": 6  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T19:30:24.662465Z",     "start_time": "2025-02-06T19:30:24.656566Z"    }   },   "cell_type": "code",   "source": [    "query = inputs[1]\n",    "context_vec_2 = torch.zeros(query.shape)\n",    "for i,x_i, in enumerate(inputs):\n",    "    context_vec_2 += attn_weights_2[i]*x_i\n",    "print(context_vec_2)"   ],   "id": "8163240b83a0939b",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([0.4419, 0.6515, 0.5683])\n"     ]    }   ],   "execution_count": 8  },  {   "metadata": {},   "cell_type": "raw",   "source": "We have computed attention weights and the context vector for input 2. Let's extend this computation to calculate attention weights and context vectors for all inputs.",   "id": "7f202e669082a180"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T19:36:05.645021Z",     "start_time": "2025-02-06T19:36:05.638737Z"    }   },   "cell_type": "code",   "source": [    "attn_scores = torch.empty(6, 6)\n",    "for i, x_i in enumerate(inputs):\n",    "    for j, x_j in enumerate(inputs):\n",    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",    "print(attn_scores)"   ],   "id": "8bef2c86cd4c61f5",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"     ]    }   ],   "execution_count": 9  },  {   "metadata": {},   "cell_type": "raw",   "source": "When computing the preceding attention score tensor, we used for loops in Python. However, for loops are generally slow, and we can achieve the same results using matrix multiplication:",   "id": "14949316ea3991b4"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T19:38:04.705408Z",     "start_time": "2025-02-06T19:38:04.699708Z"    }   },   "cell_type": "code",   "source": [    "attn_scores = inputs @ inputs.T\n",    "print(attn_scores)"   ],   "id": "c9a4ab11210cea47",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"     ]    }   ],   "execution_count": 10  },  {   "metadata": {},   "cell_type": "raw",   "source": "We normalize each row so that the values in each row sum to 1",   "id": "c8267a97c50b2bf"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T19:39:16.690745Z",     "start_time": "2025-02-06T19:39:16.685730Z"    }   },   "cell_type": "code",   "source": [    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",    "print(attn_weights)"   ],   "id": "86beeb56b4b73a70",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"     ]    }   ],   "execution_count": 11  },  {   "metadata": {},   "cell_type": "raw",   "source": "We use these attention weights to compute all context vectors via matrix multiplication:",   "id": "f4531730071ec33f"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T19:41:02.553520Z",     "start_time": "2025-02-06T19:41:02.548848Z"    }   },   "cell_type": "code",   "source": [    "all_context_vecs = attn_weights @ inputs\n",    "print(all_context_vecs)"   ],   "id": "456b4fc356f66427",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[0.4421, 0.5931, 0.5790],\n",      "        [0.4419, 0.6515, 0.5683],\n",      "        [0.4431, 0.6496, 0.5671],\n",      "        [0.4304, 0.6298, 0.5510],\n",      "        [0.4671, 0.5910, 0.5266],\n",      "        [0.4177, 0.6503, 0.5645]])\n"     ]    }   ],   "execution_count": 12  },  {   "metadata": {},   "cell_type": "markdown",   "source": [    "## Self-attention with Trainable Weights\n",    "\n",    "The most notable difference is the introduction of weight matrices that are updated during model training."   ],   "id": "9bdf2ff8fff494a"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T20:23:08.450857Z",     "start_time": "2025-02-06T20:23:08.446067Z"    }   },   "cell_type": "code",   "source": [    "x_2 = inputs[1]\n",    "d_in = inputs.shape[1]\n",    "d_out = 2"   ],   "id": "813ba05563095bf2",   "outputs": [],   "execution_count": 13  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T20:28:29.154460Z",     "start_time": "2025-02-06T20:28:29.131282Z"    }   },   "cell_type": "code",   "source": [    "torch.manual_seed(123)\n",    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"   ],   "id": "5467673f6eba8a01",   "outputs": [],   "execution_count": 14  },  {   "metadata": {},   "cell_type": "raw",   "source": "We set requires_grad=False to reduce clutter in the outputs, but if we were to use the weight matrices for model training, we would set requires_grad=True to update these matrices during model training.",   "id": "defd1a0c175f5904"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T20:29:51.378165Z",     "start_time": "2025-02-06T20:29:51.373180Z"    }   },   "cell_type": "code",   "source": [    "query_2 = x_2 @ W_query\n",    "key_2 = x_2 @ W_key\n",    "value_2 = x_2 @ W_value\n",    "\n",    "print(query_2)"   ],   "id": "4ef27e4d4ca8fb59",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([0.4306, 1.4551])\n"     ]    }   ],   "execution_count": 16  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T20:49:44.928839Z",     "start_time": "2025-02-06T20:49:44.923345Z"    }   },   "cell_type": "code",   "source": [    "keys = inputs @ W_key\n",    "values = inputs @ W_value\n",    "print(\"keys.shape:\", keys.shape)\n",    "print(\"values.shape:\", values.shape)"   ],   "id": "91ec25acdc09a3b0",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "keys.shape: torch.Size([6, 2])\n",      "values.shape: torch.Size([6, 2])\n"     ]    }   ],   "execution_count": 17  },  {   "metadata": {},   "cell_type": "raw",   "source": "First, let’s compute the attention score ω22:",   "id": "4bcf995e1cf8a60a"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T20:51:18.298888Z",     "start_time": "2025-02-06T20:51:18.294180Z"    }   },   "cell_type": "code",   "source": [    "keys_2 = keys[1]\n",    "attn_score_22 = query_2.dot(keys_2)\n",    "print(attn_score_22)"   ],   "id": "2a417273e6c28b0f",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor(1.8524)\n"     ]    }   ],   "execution_count": 18  },  {   "metadata": {},   "cell_type": "raw",   "source": "Above is the result for the unnormalized attention score",   "id": "9d5752ac277f4b06"  },  {   "metadata": {},   "cell_type": "raw",   "source": "Again, we can generalize this computation to all attention scores via matrix multiplication:",   "id": "4890e99c9ec9b6f"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T20:52:49.707643Z",     "start_time": "2025-02-06T20:52:49.703795Z"    }   },   "cell_type": "code",   "source": [    "attn_scores_2 = query_2 @ keys.T\n",    "print(attn_scores_2)"   ],   "id": "eec546a2be27dc50",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"     ]    }   ],   "execution_count": 20  },  {   "metadata": {},   "cell_type": "raw",   "source": "Now, we want to go from the attention scores to the attention weights, as illustrated in figure 3.16. We compute the attention weights by scaling the attention scores and using the softmax function. However, now we scale the attention scores by dividing them by the square root of the embedding dimension of the keys (taking the square root is mathematically the same as exponentiating by 0.5):",   "id": "92e2d6ded0f9eebe"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T20:54:19.059051Z",     "start_time": "2025-02-06T20:54:19.055078Z"    }   },   "cell_type": "code",   "source": [    "d_k = keys.shape[-1]\n",    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",    "print(attn_weights_2)"   ],   "id": "17be7917983e02e0",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"     ]    }   ],   "execution_count": 21  },  {   "metadata": {},   "cell_type": "raw",   "source": "The final step is to compute the context vectors. Now compute the context vector as a weighted sum over the value vectors.",   "id": "661fe5f8a89991c9"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T21:01:17.875245Z",     "start_time": "2025-02-06T21:01:17.868942Z"    }   },   "cell_type": "code",   "source": [    "context_vec_2 = attn_weights_2 @ values\n",    "print(context_vec_2)"   ],   "id": "db65f6cadb0b8d70",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([0.3061, 0.8210])\n"     ]    }   ],   "execution_count": 22  },  {   "metadata": {},   "cell_type": "raw",   "source": "So far, we’ve only computed a single context vector, z(2). Next, we will generalize the code to compute all context vectors in the input sequence, z(1) to z(T).",   "id": "1d8166b2b77127d6"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T21:10:41.633893Z",     "start_time": "2025-02-06T21:10:41.628895Z"    }   },   "cell_type": "code",   "source": [    "import torch.nn as nn\n",    "class SelfAttention_v1(nn.Module):\n",    "    def __init__(self, d_in, d_out):\n",    "        super().__init__()\n",    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",    "\n",    "    def forward(self, x):\n",    "        keys = x @ self.W_key\n",    "        queries = x @ self.W_query\n",    "        values = x @ self.W_value\n",    "        attn_scores = queries @ keys.T\n",    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",    "        context_vec = attn_weights @ values\n",    "        return context_vec"   ],   "id": "432e98363e65d68d",   "outputs": [],   "execution_count": 23  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T21:12:12.402334Z",     "start_time": "2025-02-06T21:12:12.394137Z"    }   },   "cell_type": "code",   "source": [    "torch.manual_seed(123)\n",    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",    "print(sa_v1(inputs))"   ],   "id": "c8b6ad32bf8c633c",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[0.2996, 0.8053],\n",      "        [0.3061, 0.8210],\n",      "        [0.3058, 0.8203],\n",      "        [0.2948, 0.7939],\n",      "        [0.2927, 0.7891],\n",      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"     ]    }   ],   "execution_count": 24  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T22:18:59.401050Z",     "start_time": "2025-02-06T22:18:59.394651Z"    }   },   "cell_type": "code",   "source": [    "class SelfAttention_v2(nn.Module):\n",    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",    "        super().__init__()\n",    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",    "\n",    "    def forward(self, x):\n",    "        keys = self.W_key(x)\n",    "        queries = self.W_query(x)\n",    "        values = self.W_value(x)\n",    "        attn_scores = queries @ keys.T\n",    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",    "        context_vec = attn_weights @ values\n",    "        return context_vec"   ],   "id": "a407cf3dbbddbda3",   "outputs": [],   "execution_count": 25  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T22:19:22.756108Z",     "start_time": "2025-02-06T22:19:22.746594Z"    }   },   "cell_type": "code",   "source": [    "torch.manual_seed(789)\n",    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",    "print(sa_v2(inputs))"   ],   "id": "b6402f0d185574b0",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[-0.0739,  0.0713],\n",      "        [-0.0748,  0.0703],\n",      "        [-0.0749,  0.0702],\n",      "        [-0.0760,  0.0685],\n",      "        [-0.0763,  0.0679],\n",      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"     ]    }   ],   "execution_count": 26  },  {   "metadata": {},   "cell_type": "markdown",   "source": [    "## Causal Attention\n",    "\n",    "Causal attention, also known as masked attention, is a specialized form of self-attention. It restricts a model to only consider previous and current inputs in a sequence when processing any given token when computing attention scores."   ],   "id": "ee336bdf183133eb"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T23:06:44.140562Z",     "start_time": "2025-02-06T23:06:44.137295Z"    }   },   "cell_type": "code",   "source": [    "queries = sa_v2.W_query(inputs)\n",    "keys = sa_v2.W_key(inputs)\n",    "attn_scores = queries @ keys.T\n",    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",    "print(attn_weights)"   ],   "id": "8645456f10901ae5",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",      "       grad_fn=<SoftmaxBackward0>)\n"     ]    }   ],   "execution_count": 27  },  {   "metadata": {},   "cell_type": "raw",   "source": "We can implement the second step using PyTorch’s tril function to create a mask where the values above the diagonal are zero:",   "id": "cc39bf5c10965384"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T23:07:36.486Z",     "start_time": "2025-02-06T23:07:36.481307Z"    }   },   "cell_type": "code",   "source": [    "context_length = attn_scores.shape[0]\n",    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",    "print(mask_simple)"   ],   "id": "919ff79cebe7d439",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[1., 0., 0., 0., 0., 0.],\n",      "        [1., 1., 0., 0., 0., 0.],\n",      "        [1., 1., 1., 0., 0., 0.],\n",      "        [1., 1., 1., 1., 0., 0.],\n",      "        [1., 1., 1., 1., 1., 0.],\n",      "        [1., 1., 1., 1., 1., 1.]])\n"     ]    }   ],   "execution_count": 28  },  {   "metadata": {},   "cell_type": "raw",   "source": "Now, we can multiply this mask with the attention weights to zero-out the values above the diagonal:",   "id": "8958fb14adcfeeff"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T23:14:51.998426Z",     "start_time": "2025-02-06T23:14:51.995655Z"    }   },   "cell_type": "code",   "source": [    "masked_simple = attn_weights * mask_simple\n",    "print(masked_simple)"   ],   "id": "72f03aff54e9247b",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",      "       grad_fn=<MulBackward0>)\n"     ]    }   ],   "execution_count": 30  },  {   "metadata": {},   "cell_type": "raw",   "source": "The third step is to renormalize the attention weights to sum up to 1 again in each row. We can achieve this by dividing each element in each row by the sum in each row:",   "id": "4930220ecbfbbb92"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T23:16:09.414098Z",     "start_time": "2025-02-06T23:16:09.409493Z"    }   },   "cell_type": "code",   "source": [    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",    "masked_simple_norm = masked_simple / row_sums\n",    "print(masked_simple_norm)"   ],   "id": "b3d61a5864cda110",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",      "       grad_fn=<DivBackward0>)\n"     ]    }   ],   "execution_count": 31  },  {   "metadata": {},   "cell_type": "markdown",   "source": [    "## Information leakage\n",    "\n",    "When we apply a mask and then renormalize the attention weights, it might initially appear that information from future tokens (which we intend to mask) could still influence the current token because their values are part of the softmax calculation. However, the key insight is that when we renormalize the attention weights after masking, what we’re essentially doing is recalculating the softmax over a smaller subset (since masked positions don’t contribute to the softmax value).\n",    "\n",    "The mathematical elegance of softmax is that despite initially including all positions in the denominator, after masking and renormalizing, the effect of the masked positions is nullified—they don’t contribute to the softmax score in any meaningful way.\n",    "\n",    "In simpler terms, after masking and renormalization, the distribution of attention weights is as if it was calculated only among the unmasked positions to begin with. This ensures there’s no information leakage from future (or otherwise masked) tokens as we intended.\n",    "\n",    "-----"   ],   "id": "5ef2aaf5c3197e25"  },  {   "metadata": {},   "cell_type": "raw",   "source": "Let’s take a mathematical property of the softmax function and implement the computation of the masked attention weights more efficiently in fewer steps",   "id": "722c2c87cf67434f"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T23:22:51.600416Z",     "start_time": "2025-02-06T23:22:51.592468Z"    }   },   "cell_type": "code",   "source": [    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",    "print(masked)"   ],   "id": "aaa1900d86144cf0",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",      "       grad_fn=<MaskedFillBackward0>)\n"     ]    }   ],   "execution_count": 32  },  {   "metadata": {},   "cell_type": "raw",   "source": "Now all we need to do is apply the softmax function to these masked results, and we are done",   "id": "b72f25b79b8c6d0f"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T23:23:36.330164Z",     "start_time": "2025-02-06T23:23:36.325730Z"    }   },   "cell_type": "code",   "source": [    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",    "print(attn_weights)"   ],   "id": "7fdf221fe8c3403f",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",      "       grad_fn=<SoftmaxBackward0>)\n"     ]    }   ],   "execution_count": 33  },  {   "metadata": {},   "cell_type": "markdown",   "source": [    "## Masking Additional Attention Weights with Dropout\n",    "\n",    "Another minor tweak to the causal attention mechanism that is useful for reducing overfitting when training LLMs.\n",    "\n",    "Dropout in deep learning is a technique where randomly selected hidden layer units are ignored during training, effectively “dropping” them out. This method helps prevent overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units. It’s important to emphasize that dropout is only used during training and is disabled afterward.\n",    "\n",    "In the transformer architecture, including models like GPT, dropout in the attention mechanism is typically applied at two specific times: after calculating the attention weights or after applying the attention weights to the value vectors. Here we will apply the dropout mask after computing the attention weights"   ],   "id": "ba5f1c4679c460f9"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T23:36:03.844902Z",     "start_time": "2025-02-06T23:36:03.833760Z"    }   },   "cell_type": "code",   "source": [    "torch.manual_seed(123)\n",    "dropout = torch.nn.Dropout(0.5)\n",    "example = torch.ones(6,6)\n",    "print(dropout(example))"   ],   "id": "cadb8cba615238c0",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[2., 2., 0., 2., 2., 0.],\n",      "        [0., 0., 0., 2., 0., 2.],\n",      "        [2., 2., 2., 2., 0., 2.],\n",      "        [0., 2., 2., 0., 0., 2.],\n",      "        [0., 2., 0., 2., 0., 2.],\n",      "        [0., 2., 2., 2., 2., 0.]])\n"     ]    }   ],   "execution_count": 34  },  {   "metadata": {},   "cell_type": "raw",   "source": [    "The values of the remaining elements in the matrix are scaled up by a factor of 1/0.5 = 2. This scaling is crucial to maintain the overall balance of the attention weights.\n",    "\n",    "Now let’s apply dropout to the attention weight matrix itself:"   ],   "id": "10efa21c76b730cb"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T23:38:50.746341Z",     "start_time": "2025-02-06T23:38:50.741899Z"    }   },   "cell_type": "code",   "source": [    "torch.manual_seed(123)\n",    "print(dropout(attn_weights))"   ],   "id": "a56cdac209799157",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",      "       grad_fn=<MulBackward0>)\n"     ]    }   ],   "execution_count": 37  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T23:50:19.131215Z",     "start_time": "2025-02-06T23:50:19.124671Z"    }   },   "cell_type": "code",   "source": [    "batch = torch.stack((inputs, inputs),dim=0)\n",    "print(batch.shape)"   ],   "id": "d69c86bb42cf7a33",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "torch.Size([2, 6, 3])\n"     ]    }   ],   "execution_count": 38  },  {   "metadata": {},   "cell_type": "raw",   "source": "This results in a three-dimensional tensor consisting of two input texts with six tokens each, where each token is a three-dimensional embedding vector",   "id": "de8b7bfa01348730"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T23:57:05.604682Z",     "start_time": "2025-02-06T23:57:05.599369Z"    }   },   "cell_type": "code",   "source": [    "class CausalAttention(nn.Module):\n",    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",    "        super().__init__()\n",    "        self.d_out = d_out\n",    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",    "        self.dropout = nn.Dropout(dropout)\n",    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",    "\n",    "    def forward(self, x):\n",    "        b, num_tokens, d_in = x.shape\n",    "        keys = self.W_key(x)\n",    "        queries = self.W_query(x)\n",    "        values = self.W_value(x)\n",    "        attn_scores = queries @ keys.transpose(1, 2)\n",    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",    "        attn_weights = self.dropout(attn_weights)\n",    "        context_vec = attn_weights @ values\n",    "        return context_vec"   ],   "id": "38234faceabd8869",   "outputs": [],   "execution_count": 39  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-06T23:59:07.673131Z",     "start_time": "2025-02-06T23:59:07.667420Z"    }   },   "cell_type": "code",   "source": [    "torch.manual_seed(123)\n",    "context_length = batch.shape[1]\n",    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",    "context_vecs = ca(batch)\n",    "print(context_vecs)\n",    "print(\"\\ncontext_vecs.shape:\", context_vecs.shape)"   ],   "id": "bc81093a792c8420",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[[-0.4519,  0.2216],\n",      "         [-0.5874,  0.0058],\n",      "         [-0.6300, -0.0632],\n",      "         [-0.5675, -0.0843],\n",      "         [-0.5526, -0.0981],\n",      "         [-0.5299, -0.1081]],\n",      "\n",      "        [[-0.4519,  0.2216],\n",      "         [-0.5874,  0.0058],\n",      "         [-0.6300, -0.0632],\n",      "         [-0.5675, -0.0843],\n",      "         [-0.5526, -0.0981],\n",      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",      "\n",      "context_vecs.shape: torch.Size([2, 6, 2])\n"     ]    }   ],   "execution_count": 41  },  {   "metadata": {},   "cell_type": "raw",   "source": "While all added code lines should be familiar at this point, we now added a self .register_buffer() call in the __init__ method. The use of register_buffer in PyTorch is not strictly necessary for all use cases but offers several advantages here. For instance, when we use the CausalAttention class in our LLM, buffers are automatically moved to the appropriate device (CPU or GPU) along with our model, which will be relevant when training our LLM. This means we don’t need to manually ensure these tensors are on the same device as your model parameters, avoiding device mismatch errors.",   "id": "be67f4748a352d36"  },  {   "metadata": {},   "cell_type": "raw",   "source": "Here’s what we’ve done so far. We began with a simplified attention mechanism, added trainable weights, and then added a causal attention mask. Next, we will extend the causal attention mechanism and code multi-head attention, which we will use in our LLM.",   "id": "bbef3663d2e5aa80"  },  {   "metadata": {},   "cell_type": "markdown",   "source": [    "## Multi-head Attention\n",    "\n",    "The term “multi-head” refers to dividing the attention mechanism into multiple “heads,” each operating independently. In this context, a single causal attention module can be considered single-head attention, where there is only one set of attention weights processing the input sequentially.\n",    "\n",    "As mentioned before, the main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections—the results of multiplying the input data (like the query, key, and value vectors in attention mechanisms) by a weight matrix. In code, we can achieve this by implementing a simple MultiHeadAttentionWrapper class that stacks multiple instances of our previously implemented CausalAttention module."   ],   "id": "f27cd59fd89e446"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-07T17:42:59.023787Z",     "start_time": "2025-02-07T17:42:59.011772Z"    }   },   "cell_type": "code",   "source": [    "class MultiHeadAttentionWrapper(nn.Module):\n",    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",    "        super().__init__()\n",    "        self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",    "\n",    "    def forward(self, x):\n",    "        return torch.cat([head(x) for head in self.heads], dim=-1)"   ],   "id": "c8552284e54c7978",   "outputs": [],   "execution_count": 42  },  {   "metadata": {},   "cell_type": "raw",   "source": [    "For example, if we use this MultiHeadAttentionWrapper class with two attention heads (via num_heads=2) and CausalAttention output dimension d_out=2, we get a four-dimensional context vector (d_out*num_heads=4)\n",    "\n",    "Let's make an example:"   ],   "id": "1424db0908bc7f9f"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-07T17:49:28.320837Z",     "start_time": "2025-02-07T17:49:28.313757Z"    }   },   "cell_type": "code",   "source": [    "torch.manual_seed(123)\n",    "context_length = batch.shape[1] #This is the number of tokens\n",    "d_in, d_out = 3, 2\n",    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",    "context_vecs = mha(batch)\n",    "print(context_vecs)\n",    "print(\"context_vecs.shape:\", context_vecs.shape)"   ],   "id": "dd66198fd1525c2e",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",      "\n",      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",      "context_vecs.shape: torch.Size([2, 6, 4])\n"     ]    }   ],   "execution_count": 50  },  {   "metadata": {},   "cell_type": "raw",   "source": [    "- The first dimension of the resulting context_vecs tensor is 2 since we have two input texts\n",    "- The second dimension refers to the 6 tokens in each input\n",    "- The third dimension refers to the four-dimensional embedding of each token."   ],   "id": "1cff111da01162d1"  },  {   "metadata": {},   "cell_type": "raw",   "source": [    "Let's make that computations simultaneously via matrix multiplication.\n",    "\n",    "So far, we have created a MultiHeadAttentionWrapper to implement multi-head attention by stacking multiple single-head attention modules. This was done by instantiating and combining several CausalAttention objects.\n",    "\n",    "Instead of maintaining two separate classes, MultiHeadAttentionWrapper and CausalAttention, we can combine these concepts into a single MultiHeadAttention class.\n",    "\n",    "The following MultiHeadAttention class integrates the multi-head functionality within a single class. It splits the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results from these heads after computing attention."   ],   "id": "f05ba48131212297"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-07T18:28:28.073104Z",     "start_time": "2025-02-07T18:28:28.063690Z"    }   },   "cell_type": "code",   "source": [    "class MultiHeadAttention(nn.Module):\n",    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",    "        super().__init__()\n",    "        assert (d_out % num_heads == 0), \\\n",    "            \"d_out must be divisible by num_heads\"\n",    "\n",    "        self.d_out = d_out\n",    "        self.num_heads = num_heads\n",    "        self.head_dim = d_out // num_heads\n",    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",    "        self.out_proj = nn.Linear(d_out, d_out)\n",    "        self.dropout = nn.Dropout(dropout)\n",    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",    "\n",    "    def forward(self, x):\n",    "        b, num_tokens, d_in = x.shape\n",    "        keys = self.W_key(x)\n",    "        queries = self.W_query(x)\n",    "        values = self.W_value(x)\n",    "\n",    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",    "\n",    "        keys = keys.transpose(1, 2)\n",    "        values = values.transpose(1, 2)\n",    "        queries = queries.transpose(1, 2)\n",    "\n",    "        attn_scores = queries @ keys.transpose(2, 3)\n",    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",    "\n",    "        attn_scores.masked_fill(mask_bool.bool(), -torch.inf)\n",    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",    "        attn_weights = self.dropout(attn_weights)\n",    "\n",    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",    "        context_vec = self.out_proj(context_vec)\n",    "\n",    "        return context_vec\n"   ],   "id": "f57474b35f6ee4ec",   "outputs": [],   "execution_count": 52  },  {   "metadata": {},   "cell_type": "markdown",   "source": "**Even though the reshaping (.view) and transposing (.transpose) of tensors inside the MultiHeadAttention class looks very mathematically complicated, the MultiHeadAttention class implements the same concept as the MultiHeadAttentionWrapper earlier.**",   "id": "f88f66736b29df35"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-07T18:35:57.618943Z",     "start_time": "2025-02-07T18:35:57.613051Z"    }   },   "cell_type": "code",   "source": [    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",    "\n",    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])"   ],   "id": "33c60fcd79169250",   "outputs": [],   "execution_count": 53  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-07T18:36:06.423811Z",     "start_time": "2025-02-07T18:36:06.419350Z"    }   },   "cell_type": "code",   "source": "print(a @ a.transpose(2, 3))",   "id": "f23d20c1d041589c",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[[[1.3208, 1.1631, 1.2879],\n",      "          [1.1631, 2.2150, 1.8424],\n",      "          [1.2879, 1.8424, 2.0402]],\n",      "\n",      "         [[0.4391, 0.7003, 0.5903],\n",      "          [0.7003, 1.3737, 1.0620],\n",      "          [0.5903, 1.0620, 0.9912]]]])\n"     ]    }   ],   "execution_count": 54  },  {   "metadata": {},   "cell_type": "raw",   "source": "Now we perform a batched matrix multiplication between the tensor itself and a view of the tensor where we transposed the last two dimensions, num_tokens and head_dim",   "id": "c5ff26e6fdcf77b8"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-07T18:56:24.782836Z",     "start_time": "2025-02-07T18:56:24.775605Z"    }   },   "cell_type": "code",   "source": [    "first_head = a[0, 0, :, :]\n",    "first_res = first_head @ first_head.T\n",    "print(\"First head:\", first_res)\n",    "\n",    "second_head = a[0, 1, :, :]\n",    "second_res = second_head @ second_head.T\n",    "print(\"\\nSecond head:\", second_res)"   ],   "id": "6994aa5b8303ae56",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "First head: tensor([[1.3208, 1.1631, 1.2879],\n",      "        [1.1631, 2.2150, 1.8424],\n",      "        [1.2879, 1.8424, 2.0402]])\n",      "\n",      "Second head: tensor([[0.4391, 0.7003, 0.5903],\n",      "        [0.7003, 1.3737, 1.0620],\n",      "        [0.5903, 1.0620, 0.9912]])\n"     ]    }   ],   "execution_count": 55  },  {   "metadata": {},   "cell_type": "markdown",   "source": [    "Continuing with MultiHeadAttention, after computing the attention weights and context vectors, the context vectors from all heads are transposed back to the shape (b, num_tokens, num_heads, head_dim). These vectors are then reshaped (flattened) into the shape (b, num_tokens, d_out), effectively combining the outputs from all heads.\n",    "\n",    "\n",    "**Even though the MultiHeadAttention class looks more complicated than the MultiHeadAttentionWrapper due to the additional reshaping and transposition of tensors, it is more efficient. The reason is that we only need one matrix multiplication to compute the keys, for instance, keys = self.W_key(x) (the same is true for the queries and values). In the MultiHeadAttentionWrapper, we needed to repeat this matrix multiplication, which is computationally one of the most expensive steps, for each attention head.**"   ],   "id": "a00b621a0ecf2dfd"  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-02-07T19:14:04.819470Z",     "start_time": "2025-02-07T19:14:04.808204Z"    }   },   "cell_type": "code",   "source": [    "torch.manual_seed(123)\n",    "batch_size, context_length, d_in = batch.shape\n",    "d_out = 2\n",    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",    "context_vecs = mha(batch)\n",    "print(context_vecs)\n",    "print(\"context_vecs.shape:\", context_vecs.shape)\n"   ],   "id": "23498fd3df4306c4",   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "tensor([[[0.2595, 0.4014],\n",      "         [0.2583, 0.4014],\n",      "         [0.2583, 0.4014],\n",      "         [0.2575, 0.4031],\n",      "         [0.2582, 0.4026],\n",      "         [0.2575, 0.4028]],\n",      "\n",      "        [[0.2595, 0.4014],\n",      "         [0.2583, 0.4014],\n",      "         [0.2583, 0.4014],\n",      "         [0.2575, 0.4031],\n",      "         [0.2582, 0.4026],\n",      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",      "context_vecs.shape: torch.Size([2, 6, 2])\n"     ]    }   ],   "execution_count": 57  },  {   "metadata": {},   "cell_type": "markdown",   "source": [    "## Summary\n",    "- **Attention mechanisms transform input elements into enhanced context vector representations that incorporate information about all inputs.**\n",    "- A self-attention mechanism computes the context vector representation as a weighted sum over the inputs.\n",    "- In a simplified attention mechanism, the attention weights are computed via dot products.\n",    "- A dot product is a concise way of multiplying two vectors element-wise and then summing the products.\n",    "- Matrix multiplications, while not strictly required, help us implement computations more efficiently and compactly by replacing nested for loops.\n",    "- In self-attention mechanisms used in LLMs, also called scaled-dot product attention, we include trainable weight matrices to compute intermediate transformations of the inputs: queries, values, and keys.\n",    "- When working with LLMs that read and generate text from left to right, we add a causal attention mask to prevent the LLM from accessing future tokens.\n",    "- In addition to causal attention masks to zero-out attention weights, we can add a **dropout mask to reduce overfitting** in LLMs.\n",    "- The attention modules in transformer-based LLMs involve multiple instances of causal attention, which is called multi-head attention.\n",    "- We can create a multi-head attention module by stacking multiple instances of causal attention modules.\n",    "- A more efficient way of creating multi-head attention modules involves batched matrix multiplications."   ],   "id": "11256aa8cdda76ca"  } ], "metadata": {  "kernelspec": {   "display_name": "Python 3",   "language": "python",   "name": "python3"  },  "language_info": {   "codemirror_mode": {    "name": "ipython",    "version": 2   },   "file_extension": ".py",   "mimetype": "text/x-python",   "name": "python",   "nbconvert_exporter": "python",   "pygments_lexer": "ipython2",   "version": "2.7.6"  } }, "nbformat": 4, "nbformat_minor": 5}